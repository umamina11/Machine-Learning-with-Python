{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a818bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#A decision tree is a supervised learning algorithm used for both classification and regression tasks in machine learning. It is a tree-like structure where each internal node represents a feature, the branch represents a decision rule based on that feature, and each leaf node represents the outcome. Decision trees are intuitive and easy to understand, making them popular for both analysis and interpretation of data.\n",
    "\n",
    "#Here's how a decision tree works:\n",
    "\n",
    "\"\"\"1. Training Phase:\n",
    "   - Given a dataset with features and corresponding labels, the decision tree algorithm recursively splits the dataset into subsets based on the values of the features.\n",
    "   - At each step, the algorithm selects the best feature to split the data. The \"best\" feature is chosen based on a criterion that maximizes the information gain or minimizes impurity.\n",
    "   - Common splitting criteria include Gini impurity, entropy, or classification error for classification tasks, and variance reduction for regression tasks.\n",
    "   - This process continues recursively until one of the stopping conditions is met, such as reaching a maximum depth, having a minimum number of samples in a node, or no further improvement in impurity reduction.\n",
    "\n",
    "2. Testing Phase:\n",
    "   - Once the decision tree is trained, it can be used to make predictions on new, unseen data.\n",
    "   - Given an input instance, the decision tree traverses from the root node down to a leaf node following the decision rules based on the features' values.\n",
    "   - At each internal node, the decision tree evaluates the feature value and moves to the appropriate child node according to the decision rule.\n",
    "   - When reaching a leaf node, the prediction is made based on the majority class in the case of classification or the mean value in the case of regression.\n",
    "\n",
    "Decision trees have several advantages:\n",
    "\n",
    "- Interpretability: Decision trees are easy to interpret and visualize, making them useful for understanding the decision-making process.\n",
    "- Non-parametric: Decision trees make no assumptions about the underlying distribution of the data, making them suitable for both linear and non-linear relationships.\n",
    "- Handle Both Numerical and Categorical Data: Decision trees can handle both numerical and categorical features without requiring pre-processing like one-hot encoding.\n",
    "- Feature Importance: Decision trees can provide information about feature importance, which can be helpful for feature selection and understanding the importance of different variables in predicting the target variable.\n",
    "\n",
    "However, decision trees also have limitations:\n",
    "\n",
    "- Overfitting: Decision trees are prone to overfitting, especially when the tree depth is not properly controlled or when the dataset is noisy.\n",
    "- Instability: Small changes in the data can result in significantly different trees, leading to instability.\n",
    "- Bias towards Features with Many Levels**: Decision trees tend to bias towards features with a large number of levels or categories.\n",
    "- Single Decision Boundaries: Decision trees create axis-parallel decision boundaries, which may not capture complex relationships in the data.\n",
    "\n",
    "To address some of these limitations, ensemble methods like random forests and gradient boosting are often used, which combine multiple decision trees to improve performance and robustness.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b600b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33201e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_data = pd.read_csv(\"wines_SPA.csv\", sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d376f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3051999",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "g = sns.pairplot(data=wine_data, diag_kind=\"kde\", dropna=True)\n",
    "g.map_lower(sns.kdeplot, levels=4, color=\".2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db48fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac2ddf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "summary_stats = wine_data.describe()\n",
    "print(summary_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a460e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of the target variable\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.histplot(wine_data['rating'], kde=True, bins=20, color='skyblue')\n",
    "plt.xlabel('Rating')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Wine Ratings')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb4baed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix\n",
    "correlation_matrix = wine_data.corr()\n",
    "\n",
    "# Plot the correlation matrix as a heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f')\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a389a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pairplot\n",
    "sns.pairplot(wine_data, vars=['rating', 'num_reviews', 'price', 'body', 'acidity'])\n",
    "plt.suptitle('Pairplot of Numerical Features', y=1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42991e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplot of numerical features by wine type\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.boxplot(data=wine_data, x='type', y='rating')\n",
    "plt.xlabel('Wine Type')\n",
    "plt.ylabel('Rating')\n",
    "plt.title('Distribution of Ratings by Wine Type')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d37b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Countplot of wine types\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.countplot(data=wine_data, x='type')\n",
    "plt.xlabel('Wine Type')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Count of Wine Types')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9bd89eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc563789",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6cdc48",
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_data.fillna(method='ffill', inplace=True)  # Forward fill missing values\n",
    "\n",
    "# Convert categorical variables into numerical format using one-hot encoding\n",
    "wine_data = pd.get_dummies(wine_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fee640d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = wine_data.drop('rating', axis=1)  # Features\n",
    "y = wine_data['rating']  # Target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2cbcc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0bc5c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Initialize the decision tree model\n",
    "dtree = DecisionTreeRegressor()\n",
    "\n",
    "# Train the model\n",
    "dtree.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d85faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Predictions on the testing set\n",
    "y_pred = dtree.predict(X_test)\n",
    "\n",
    "# Calculate Mean Squared Error\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print('Mean Squared Error:', mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2775c0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import plot_tree\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Convert Index object to a list\n",
    "feature_names = list(X.columns)\n",
    "\n",
    "# Plot the decision tree\n",
    "plt.figure(figsize=(20,10))\n",
    "plot_tree(dtree, feature_names=feature_names, filled=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150448b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define rating categories\n",
    "low_threshold = 3.5\n",
    "high_threshold = 4.5\n",
    "\n",
    "# Create a new categorical target variable based on rating categories\n",
    "wine_data['rating_category'] = pd.cut(wine_data['rating'], bins=[0, low_threshold, high_threshold, 5], labels=['Low', 'Medium', 'High'])\n",
    "\n",
    "# Split the data into features (X) and the new categorical target variable (y)\n",
    "X = wine_data.drop(['rating', 'rating_category'], axis=1)\n",
    "y = wine_data['rating_category']\n",
    "\n",
    "# Perform train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the decision tree model\n",
    "dtree = DecisionTreeClassifier()\n",
    "dtree.fit(X_train, y_train)\n",
    "\n",
    "# Predictions on the testing set\n",
    "y_pred = dtree.predict(X_test)\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "# Plot the confusion matrix as a heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e20c5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776bd1cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760fb0a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b549375d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
